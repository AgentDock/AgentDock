/**
 * @fileoverview LLM-specific error handling utilities
 * Provides standardized error patterns for different LLM providers
 */

import { LLMProvider } from '../llm/types';
import { LogCategory, logger } from '../logging';
import { AgentError, createError } from './index';

/**
 * Error codes used in this module
 * These must match the ErrorCode enum values in index.ts
 */
const ErrorCodes = {
  LLM_RATE_LIMIT: 'LLM_RATE_LIMIT_ERROR',
  LLM_EXECUTION: 'LLM_EXECUTION_ERROR',
  LLM_API_KEY: 'LLM_API_KEY_ERROR',
  LLM_OVERLOADED: 'LLM_OVERLOADED_ERROR',
  SERVICE_UNAVAILABLE: 'SERVICE_UNAVAILABLE',
  UNKNOWN: 'UNKNOWN_ERROR'
};

/**
 * Interface for mapping provider-specific error patterns to standardized error codes
 */
interface ProviderErrorPattern {
  pattern: RegExp | string;
  errorCode: string;
  statusCode?: number;
  userMessage?: string;
}

/**
 * Map of provider-specific error patterns
 * Each provider has a map of patterns that identify specific error types
 */
const ERROR_PATTERNS: Record<LLMProvider | 'byok', ProviderErrorPattern[]> = {
  // Anthropic error patterns
  anthropic: [
    {
      pattern: /rate\s*limit|would\s*exceed\s*the\s*rate\s*limit/i,
      errorCode: ErrorCodes.LLM_RATE_LIMIT,
      statusCode: 429,
      userMessage:
        "You've reached the rate limit for Anthropic API. Please try again later."
    },
    {
      pattern: 'Overloaded',
      errorCode: ErrorCodes.LLM_OVERLOADED,
      statusCode: 529,
      userMessage:
        'Anthropic service is temporarily overloaded. Please try again shortly.'
    },
    {
      pattern: /context\s*window|token\s*limit/i,
      errorCode: ErrorCodes.LLM_EXECUTION,
      statusCode: 400,
      userMessage:
        "The prompt exceeds Anthropic's maximum context length. Please reduce your input."
    },
    {
      pattern: /invalid\s*api\s*key|api\s*key\s*invalid|authentication/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage: 'Invalid Anthropic API key. Please check your credentials.'
    },
    {
      pattern: /service\s*unavailable|server\s*error|5\d\d/i,
      errorCode: ErrorCodes.SERVICE_UNAVAILABLE,
      statusCode: 503,
      userMessage:
        'Anthropic service is currently unavailable. Please try again later.'
    }
  ],

  // OpenAI error patterns
  openai: [
    {
      pattern: /rate\s*limit/i,
      errorCode: ErrorCodes.LLM_RATE_LIMIT,
      statusCode: 429,
      userMessage:
        "You've reached the rate limit for OpenAI API. Please try again later."
    },
    {
      pattern: /context\s*length|maximum\s*context\s*length|token\s*limit/i,
      errorCode: ErrorCodes.LLM_EXECUTION,
      statusCode: 400,
      userMessage:
        "The prompt exceeds OpenAI's maximum context length. Please reduce your input."
    },
    {
      pattern: /invalid\s*api\s*key|api\s*key\s*invalid|authentication/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage: 'Invalid OpenAI API key. Please check your credentials.'
    },
    {
      pattern: /service\s*unavailable|server\s*error|5\d\d/i,
      errorCode: ErrorCodes.SERVICE_UNAVAILABLE,
      statusCode: 503,
      userMessage:
        'OpenAI service is currently unavailable. Please try again later.'
    }
  ],

  // Google (Gemini) error patterns
  gemini: [
    {
      pattern: /rate\s*limit/i,
      errorCode: ErrorCodes.LLM_RATE_LIMIT,
      statusCode: 429,
      userMessage:
        "You've reached the rate limit for Google Gemini API. Please try again later."
    },
    {
      pattern: /content\s*filter|blocked\s*content/i,
      errorCode: ErrorCodes.LLM_EXECUTION,
      statusCode: 400,
      userMessage:
        "Your request was blocked by Google's content filter. Please modify your input."
    },
    {
      pattern: /invalid\s*api\s*key|api\s*key\s*invalid|authentication/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage: 'Invalid Google API key. Please check your credentials.'
    },
    {
      pattern: /service\s*unavailable|server\s*error|5\d\d/i,
      errorCode: ErrorCodes.SERVICE_UNAVAILABLE,
      statusCode: 503,
      userMessage:
        'Google Gemini service is currently unavailable. Please try again later.'
    }
  ],

  // DeepSeek error patterns
  deepseek: [
    {
      pattern: /rate\s*limit/i,
      errorCode: ErrorCodes.LLM_RATE_LIMIT,
      statusCode: 429,
      userMessage:
        "You've reached the rate limit for DeepSeek API. Please try again later."
    },
    {
      pattern: /context\s*length|maximum\s*context\s*length|token\s*limit/i,
      errorCode: ErrorCodes.LLM_EXECUTION,
      statusCode: 400,
      userMessage:
        "The prompt exceeds DeepSeek's maximum context length. Please reduce your input."
    },
    {
      pattern: /invalid\s*api\s*key|api\s*key\s*invalid|authentication/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage: 'Invalid DeepSeek API key. Please check your credentials.'
    }
  ],

  // Groq error patterns
  groq: [
    {
      pattern: /rate\s*limit/i,
      errorCode: ErrorCodes.LLM_RATE_LIMIT,
      statusCode: 429,
      userMessage:
        "You've reached the rate limit for Groq API. Please try again later."
    },
    {
      pattern: /context\s*length|maximum\s*context\s*length|token\s*limit/i,
      errorCode: ErrorCodes.LLM_EXECUTION,
      statusCode: 400,
      userMessage:
        "The prompt exceeds Groq's maximum context length. Please reduce your input."
    },
    {
      pattern: /invalid\s*api\s*key|api\s*key\s*invalid|authentication/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage: 'Invalid Groq API key. Please check your credentials.'
    }
  ],

  // BYOK mode error patterns
  byok: [
    {
      pattern: /api\s*key\s*required|no\s*api\s*key/i,
      errorCode: ErrorCodes.LLM_API_KEY,
      statusCode: 401,
      userMessage:
        'API key required. In BYOK mode, you must provide your own API key.'
    }
  ]
};

/**
 * Parse provider-specific errors and convert them to standardized AgentError objects
 * @param error The original error from the provider
 * @param provider The LLM provider (anthropic, openai, etc.)
 * @param isByokMode Whether the system is in BYOK mode (bring your own key)
 * @returns An AgentError with standardized error code and message
 */
export function parseProviderError(
  error: unknown,
  provider: LLMProvider,
  isByokMode: boolean = false
): AgentError {
  // === START DEBUG LOGGING ===
  logger.debug(
    LogCategory.LLM,
    'parseProviderError',
    'Raw error object received',
    {
      rawError: JSON.stringify(error, Object.getOwnPropertyNames(error)), // Log all properties
      errorType: typeof error,
      isErrorInstance: error instanceof Error
    }
  );
  // === END DEBUG LOGGING ===

  // Get error message as string - REVISED EXTRACTION LOGIC
  let errorMessage = 'Unknown error';
  let parsedErrorObj: any = null;

  try {
    if (error instanceof Error) {
      errorMessage = error.message;
      parsedErrorObj = error; // Keep the original error object
    } else if (typeof error === 'string') {
      // Try to parse if it looks like JSON
      if (error.trim().startsWith('{') && error.trim().endsWith('}')) {
        try {
          parsedErrorObj = JSON.parse(error);
          if (
            parsedErrorObj &&
            typeof parsedErrorObj === 'object' &&
            parsedErrorObj.message
          ) {
            errorMessage = String(parsedErrorObj.message);
          } else {
            errorMessage = error; // Fallback to original string if parse/extraction fails
          }
        } catch (e) {
          errorMessage = error; // Fallback if JSON parsing fails
        }
      } else {
        errorMessage = error; // It's just a plain string
        parsedErrorObj = { message: error };
      }
    } else if (error && typeof error === 'object') {
      // Handle plain objects or nested errors
      parsedErrorObj = error;
      // Check if message property exists and is string-like before accessing
      if (
        'message' in error &&
        typeof (error as any).message === 'string' &&
        (error as any).message
      ) {
        errorMessage = String((error as any).message);
      } else if ('error' in error && error.error) {
        // Handle cases where error is nested like { error: { message: ... } } or { error: "..." }
        const nestedError = error.error;
        // Check if nested message exists and is string-like
        if (
          nestedError &&
          typeof nestedError === 'object' &&
          'message' in nestedError &&
          typeof (nestedError as any).message === 'string' &&
          (nestedError as any).message
        ) {
          errorMessage = String((nestedError as any).message);
        } else if (typeof nestedError === 'string') {
          errorMessage = nestedError;
        } else {
          errorMessage = JSON.stringify(error); // Fallback for complex objects
        }
      } else if ('response' in error && error.response) {
        // Axios-like errors
        const response = (error as any).response;
        if (response.data?.error?.message) {
          errorMessage = String(response.data.error.message);
        } else if (response.data?.error) {
          errorMessage = String(response.data.error);
        } else if (response.data) {
          errorMessage =
            typeof response.data === 'string'
              ? response.data
              : JSON.stringify(response.data);
        } else {
          errorMessage = `HTTP Error ${response.status}`;
        }
      } else {
        errorMessage = JSON.stringify(error); // Fallback stringify
      }
    }
  } catch (extractionError) {
    // Fallback in case the extraction logic itself throws an error
    errorMessage = 'Failed to process error details.';
    logger.warn(
      LogCategory.LLM,
      'parseProviderError',
      'Error during error message extraction',
      { extractionError }
    );
  }

  // Handle missing API key in BYOK mode first
  if (
    isByokMode &&
    /no\s*api\s*key|api\s*key\s*required|api\s*key\s*missing/i.test(
      errorMessage
    )
  ) {
    const byokPatterns = ERROR_PATTERNS.byok;
    const matchingPattern = byokPatterns.find((pattern) =>
      pattern.pattern instanceof RegExp
        ? pattern.pattern.test(errorMessage)
        : errorMessage.includes(pattern.pattern)
    );

    if (matchingPattern) {
      return createError(
        'llm',
        matchingPattern.userMessage || 'API key required in BYOK mode',
        matchingPattern.errorCode as any,
        { provider, originalError: error },
        matchingPattern.statusCode || 401
      );
    }
  }

  // Check provider-specific patterns
  const providerPatterns = ERROR_PATTERNS[provider] || [];

  // Trim the errorMessage before matching to handle potential hidden whitespace/characters
  const trimmedErrorMessage = errorMessage.trim();

  const matchingPattern = providerPatterns.find((pattern) =>
    pattern.pattern instanceof RegExp
      ? pattern.pattern.test(trimmedErrorMessage)
      : trimmedErrorMessage.includes(pattern.pattern)
  );

  // Log the error for debugging
  logger.debug(
    LogCategory.LLM,
    'parseProviderError',
    'Parsing provider error',
    {
      provider,
      originalErrorMessage: errorMessage,
      trimmedErrorMessage: trimmedErrorMessage,
      matchedPattern: matchingPattern?.pattern.toString() || 'none'
    }
  );

  // Return structured error
  if (matchingPattern) {
    return createError(
      'llm',
      matchingPattern.userMessage || errorMessage,
      matchingPattern.errorCode as any,
      { provider, originalError: error },
      matchingPattern.statusCode || 500
    );
  }

  // Default case - unknown LLM error
  return createError(
    'llm',
    `Error from ${provider}: ${errorMessage}`,
    ErrorCodes.LLM_EXECUTION as any,
    { provider, originalError: error },
    500
  );
}

/**
 * Error response format expected by Vercel AI SDK
 */
export interface ErrorResponse {
  error: string;
  code: string;
  details?: Record<string, any>;
}

/**
 * Normalizes any error into a properly formatted error response object
 * Ensures that error messages are always strings as required by Vercel AI SDK
 */
export function normalizeError(error: unknown): ErrorResponse {
  let errorMessage = 'An unknown error occurred';
  let errorCode: string = ErrorCodes.UNKNOWN;
  let errorDetails: Record<string, any> = {};

  // Handle different error types
  if (error instanceof Error) {
    errorMessage = error.message;

    // Handle our custom error types
    if ('code' in error && typeof error.code === 'string') {
      errorCode = error.code;
    }

    // Extract additional details if available
    if ('details' in error && typeof error.details === 'object') {
      errorDetails = error.details as Record<string, any>;
    }
  } else if (typeof error === 'string') {
    errorMessage = error;
  } else if (error && typeof error === 'object') {
    // Handle plain object errors
    if ('message' in error && error.message) {
      errorMessage = String(error.message);
    }

    if ('code' in error && error.code) {
      errorCode = String(error.code);
    }

    // Extract any details
    if ('details' in error) {
      errorDetails = { ...((error.details as Record<string, any>) || {}) };
    }

    // Extract from nested error objects (like Axios errors)
    if ('response' in error && (error as any).response) {
      const response = (error as any).response;

      if (response.data?.error?.message) {
        errorMessage = String(response.data.error.message);
      }

      if (response.data?.error?.type) {
        errorDetails.errorType = String(response.data.error.type);
      }

      errorDetails.status = response.status;
    }
  }

  // Map specific errors to user-friendly messages
  if (errorCode === ErrorCodes.LLM_API_KEY) {
    if (
      errorMessage.includes('BYOK') ||
      errorMessage.toLowerCase().includes('bring your own')
    ) {
      errorMessage =
        'API key required. In BYOK mode, you must provide your own API key.';
    } else {
      errorMessage = 'API key required. Please add your API key in settings.';
    }
  } else if (errorCode === ErrorCodes.LLM_RATE_LIMIT) {
    errorMessage = 'Rate limit exceeded. Please try again later.';
  } else if (errorCode === ErrorCodes.SERVICE_UNAVAILABLE) {
    errorMessage = 'Service currently unavailable. Please try again later.';
  }

  // Always ensure the error is a string to prevent "error parts expect a string value"
  return {
    error: String(errorMessage),
    code: String(errorCode),
    details: process.env.NODE_ENV === 'development' ? errorDetails : undefined
  };
}
